"""
Enhanced Mexican Municipal Candidates Scraper main module.

This module provides the main scraper class and entry point for the application
with improved database repair and error handling.
"""
import os
import sys
import json
import time
import pandas as pd
import argparse
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import traceback

# Add the project directory to the path
project_root = Path(__file__).resolve().parent
sys.path.append(str(project_root))

# Import project modules
from utils.logger import get_logger, setup_logger
from config.settings import (
    DEFAULT_DB_PATH, OXYLABS_USERNAME, OXYLABS_PASSWORD, OXYLABS_COUNTRY,
    DEFAULT_THREADS, ARTICLE_THREADS, DEFAULT_YEAR_RANGE, RESULTS_DIR,
    MIN_RELEVANCE_THRESHOLD
)
from database.db_manager import DatabaseManager
from scrapers.oxylabs_manager import OxylabsAPIManager
from scrapers.search_engine import create_search_engine
from processing.content_classifier import create_content_classifier
from processing.entity_recognizer import create_entity_recognizer
from processing.name_matcher import create_name_matcher  # Add this line

# Set up logger
logger = setup_logger()

def parse_arguments():
    """
    Parse command line arguments with enhanced data validation options.
    
    Returns:
        argparse.Namespace: Parsed arguments
    """
    parser = argparse.ArgumentParser(description='Enhanced Mexican Municipal Candidates Web Scraper')
    
    # Input and database options
    parser.add_argument('--csv', '-c', type=str, help='Path to candidates CSV file')
    parser.add_argument('--db', '-d', type=str, default=DEFAULT_DB_PATH, 
                      help='Path to SQLite database')
    
    # Processing options
    parser.add_argument('--threads', '-t', type=int, default=DEFAULT_THREADS, 
                      help='Maximum number of concurrent threads for candidates')
    parser.add_argument('--article-threads', '-a', type=int, default=ARTICLE_THREADS,
                      help='Maximum number of concurrent threads for articles')
    parser.add_argument('--year-range', '-y', type=int, default=DEFAULT_YEAR_RANGE, 
                      help='Year range for temporal filtering (Â±)')
    parser.add_argument('--max-candidates', '-m', type=int, default=0, 
                      help='Maximum number of candidates to process (0 for all)')
    
    # Scraping behavior
    parser.add_argument('--no-oxylabs', action='store_true', 
                      help='Disable Oxylabs API')
    parser.add_argument('--enhanced-search', action='store_true', 
                      help='Use enhanced search strategies')
    
    # Output options
    parser.add_argument('--export', '-e', action='store_true', 
                      help='Export results after processing')
    parser.add_argument('--export-format', '-f', type=str, default='json', 
                      choices=['csv', 'json'], help='Export format (csv or json)')
    parser.add_argument('--min-relevance', '-r', type=float, default=MIN_RELEVANCE_THRESHOLD, 
                      help='Minimum relevance threshold for exports')
    parser.add_argument('--output-path', type=str, default=RESULTS_DIR,
                      help='Path for exported files')
    
    # Utility operations
    parser.add_argument('--create-dataset', action='store_true', 
                      help='Create a consolidated dataset for ML/NLP')
    parser.add_argument('--dataset-format', type=str, default='json', 
                      choices=['json', 'csv'], help='Dataset format')
    parser.add_argument('--create-profiles', action='store_true',
                      help='Create candidate profiles without running scraper')
    parser.add_argument('--extract-batch', type=int, default=None,
                      help='Export results for a specific batch')
    
    # Data validation options
    parser.add_argument('--analyze-data', action='store_true',
                      help='Analyze candidates data without running scraper')
    parser.add_argument('--validate-db', action='store_true',
                      help='Validate the database structure and contents')
    parser.add_argument('--clean-only', action='store_true',
                      help='Clean and validate data without running scraper')
    
    # Database repair options (new)
    parser.add_argument('--repair-db', action='store_true',
                      help='Attempt to repair common database issues')
    parser.add_argument('--force-rebuild', action='store_true',
                      help='Force database schema rebuild (use with caution)')
                      
    # Excel export option
    parser.add_argument('--export-excel', action='store_true', 
                  help='Export results to Excel format')
                      
    return parser.parse_args()

def main():
    """
    Main function with enhanced data validation and error handling.
    """
    # Parse arguments
    args = parse_arguments()
    
    try:
        # Run database repair if requested
        if args.repair_db:
            logger.info("Repairing database...")
            db_manager = DatabaseManager(args.db)
            repairs = db_manager.repair_database()
            logger.info(f"Database repair completed: {repairs}")
            return
            
        # Force database rebuild if requested
        if args.force_rebuild:
            logger.warning("Forcing database schema rebuild...")
            # First backup the database
            import shutil
            from datetime import datetime
            
            backup_path = f"{args.db}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            try:
                shutil.copy2(args.db, backup_path)
                logger.info(f"Database backed up to {backup_path}")
            except Exception as e:
                logger.error(f"Backup failed: {str(e)}. Aborting rebuild.")
                return
                
            # Connect directly to database to rebuild schema
            import sqlite3
            from database.models import Schema
            
            try:
                conn = sqlite3.connect(args.db)
                cursor = conn.cursor()
                
                # Drop problematic tables and recreate them
                for table in ["articles", "candidate_articles", "quotes", "entities"]:
                    cursor.execute(f"DROP TABLE IF EXISTS {table}")
                
                # Recreate tables
                for table_name, create_sql in Schema.TABLES.items():
                    if table_name in ["articles", "candidate_articles", "quotes", "entities"]:
                        cursor.execute(create_sql)
                
                # Recreate indexes
                for index_sql in Schema.INDEXES:
                    cursor.execute(index_sql)
                    
                conn.commit()
                conn.close()
                logger.info("Database schema rebuilt successfully")
            except Exception as e:
                logger.error(f"Schema rebuild failed: {str(e)}")
                logger.info(f"You can restore from backup: {backup_path}")
                return
                
            return
        
        # Initialize scraper
        scraper = MexicanCandidateScraper(
            db_path=args.db,
            candidates_csv=args.csv,
            max_threads=args.threads,
            article_threads=args.article_threads,
            year_range=args.year_range,
            use_oxylabs=not args.no_oxylabs,
            enhanced_search=args.enhanced_search
        )
        
        # Run quick repair to fix common issues
        logger.info("Validating database consistency...")
        try:
            # Perform a lightweight repair to fix basic issues
            scraper.db.repair_common_issues()
        except Exception as e:
            logger.warning(f"Database validation error: {str(e)}")
        
        # Run data validation if requested
        if args.analyze_data:
            logger.info("Analyzing candidates data...")
            scraper.analyze_candidates_data()
            return
            
        if args.validate_db:
            logger.info("Validating database...")
            scraper.validate_database()
            return
            
        if args.clean_only:
            logger.info("Cleaning and validating data...")
            scraper.analyze_candidates_data()
            return
        
        batch_id = None
        
        # Create profiles only if requested
        if args.create_profiles:
            logger.info("Creating candidate profiles...")
            profiles_created = scraper.create_profiles(min_relevance=args.min_relevance)
            logger.info(f"Created/updated {profiles_created} candidate profiles")
        # Export specific batch if requested
        elif args.extract_batch is not None:
            logger.info(f"Exporting results for batch {args.extract_batch}")
            export_paths = scraper.export_results(
                output_path=args.output_path,
                format=args.export_format,
                min_relevance=args.min_relevance,
                batch_id=args.extract_batch
            )
            if export_paths:
                logger.info(f"Exported files: {export_paths}")
            else:
                logger.warning("No results were exported")
                
            # Also export to Excel if requested
            if args.export_excel:
                logger.info(f"Exporting results to Excel for batch {args.extract_batch}...")
                excel_path = scraper.export_results_to_excel(
                    output_path=args.output_path,
                    min_relevance=args.min_relevance,
                    batch_id=args.extract_batch
                )
                if excel_path:
                    logger.info(f"Exported Excel file: {excel_path}")
                else:
                    logger.warning("No results were exported to Excel")
        else:
            # Run batch processing
            logger.info("Starting batch processing...")
            batch_id = scraper.run_batch(max_candidates=args.max_candidates)
            
            if batch_id is None:
                logger.warning("Batch processing did not complete successfully")
        
        # Export results if requested
        if args.export and batch_id:
            logger.info(f"Exporting results for batch {batch_id}...")
            export_paths = scraper.export_results(
                output_path=args.output_path,
                format=args.export_format,
                min_relevance=args.min_relevance,
                batch_id=batch_id
            )
            if export_paths:
                logger.info(f"Exported files: {export_paths}")
            else:
                logger.warning("No results were exported")
        
        # Create ML dataset if requested
        if args.create_dataset:
            logger.info("Creating ML dataset...")
            dataset_path = scraper.export_ml_dataset(
                args.output_path,
                args.dataset_format,
                args.min_relevance
            )
            if dataset_path:
                logger.info(f"Created ML dataset: {dataset_path}")
            else:
                logger.warning("Failed to create ML dataset")
        
        logger.info("Scraping completed!")
        
        # Export to Excel if requested
        if args.export_excel and batch_id:
            logger.info(f"Exporting results to Excel for batch {batch_id}...")
            excel_path = scraper.export_results_to_excel(
                output_path=args.output_path,
                min_relevance=args.min_relevance,
                batch_id=batch_id
            )
            if excel_path:
                logger.info(f"Exported Excel file: {excel_path}")
            else:
                logger.warning("No results were exported to Excel")
        
    except Exception as e:
        logger.error(f"Critical error in main function: {str(e)}")
        traceback.print_exc()
        sys.exit(1)

class MexicanCandidateScraper:
    """
    Main class for the Mexican municipal candidate scraper.
    """
    
    def __init__(self, db_path=DEFAULT_DB_PATH, candidates_csv=None, 
            max_threads=DEFAULT_THREADS, article_threads=ARTICLE_THREADS, 
            year_range=DEFAULT_YEAR_RANGE, use_oxylabs=True, enhanced_search=True):
        """
        Initialize the scraper.
        
        Args:
            db_path (str, optional): Path to SQLite database.
            candidates_csv (str, optional): Path to candidates CSV file.
            max_threads (int, optional): Maximum number of concurrent threads for candidates.
            article_threads (int, optional): Maximum number of concurrent threads for articles.
            year_range (int, optional): Year range for temporal filtering.
            use_oxylabs (bool, optional): Whether to use Oxylabs for searches.
            enhanced_search (bool, optional): Whether to use enhanced search strategies.
        """
        self.db_path = db_path
        self.candidates_csv = candidates_csv
        self.max_threads = max_threads
        self.article_threads = article_threads
        self.year_range = year_range
        self.use_oxylabs = use_oxylabs
        self.enhanced_search = enhanced_search
        
        # Initialize database manager
        logger.info(f"Initializing database at {db_path}")
        self.db = DatabaseManager(db_path)
        
        # Initialize Oxylabs manager if enabled
        if self.use_oxylabs:
            logger.info("Initializing Oxylabs API manager")
            self.oxylabs = OxylabsAPIManager(OXYLABS_USERNAME, OXYLABS_PASSWORD, country=OXYLABS_COUNTRY)
        else:
            self.oxylabs = None
        
        # Load candidates data
        self.candidates_data = self._load_candidates()
        
        # Set up name matcher
        from processing.name_matcher import create_name_matcher
        self.name_matcher = create_name_matcher()
        
        # Set up entity recognizer with candidates data
        self.entity_recognizer = create_entity_recognizer(
            candidates_data=self.candidates_data,
            name_matcher=self.name_matcher
        )
        
        # Set up content classifier
        self.content_classifier = create_content_classifier(
            name_matcher=self.name_matcher
        )
        
        # Initialize search engine
        self.search_engine = create_search_engine(
            self.db, 
            self.oxylabs,
            content_classifier=self.content_classifier,
            entity_recognizer=self.entity_recognizer,
            year_range=self.year_range,
            max_workers=self.article_threads
        )
    
    def _load_candidates(self):
        """
        Load candidates data from CSV with comprehensive data cleaning and validation.
        
        Returns:
            pandas.DataFrame or None: Cleaned candidates data or None if loading fails
        """
        if self.candidates_csv:
            try:
                # Try to read with pandas
                df = pd.read_csv(self.candidates_csv, encoding='utf-8')
                
                # Data cleaning and validation
                logger.info(f"Raw data loaded: {len(df)} rows")
                
                # Essential columns that must have values
                essential_columns = ['PRESIDENTE_MUNICIPAL', 'MUNICIPIO', 'Year']
                
                # Verify all essential columns exist
                missing_columns = [col for col in essential_columns if col not in df.columns]
                if missing_columns:
                    logger.error(f"Missing essential columns in CSV: {missing_columns}")
                    return None
                
                # Replace empty strings with NaN for consistent handling
                df = df.replace('', pd.NA)
                
                # Count rows with missing data in essential columns
                missing_data_count = df[df[essential_columns].isna().any(axis=1)].shape[0]
                logger.info(f"Rows with missing essential data: {missing_data_count}")
                
                # Drop rows with missing essential data
                df = df.dropna(subset=essential_columns)
                logger.info(f"Rows after dropping incomplete entries: {len(df)}")
                
                # Convert string columns to strings (handles numeric values)
                string_columns = [
                    'PRESIDENTE_MUNICIPAL', 'MUNICIPIO', 'ENTIDAD', 'PARTIDO', 
                    'PERIODO_FORMATO_ORIGINAL', 'SEXO'
                ]
                
                for col in string_columns:
                    if col in df.columns:
                        # Convert to string, but handle NaN values first
                        df[col] = df[col].fillna('').astype(str)
                        # Convert empty strings back to NaN
                        df[col] = df[col].replace('', pd.NA)
                        # Strip whitespace
                        df[col] = df[col].str.strip()
                
                # Ensure Year is numeric
                if 'Year' in df.columns:
                    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
                    # Drop rows with invalid years
                    invalid_years = df['Year'].isna().sum()
                    if invalid_years > 0:
                        logger.warning(f"Dropping {invalid_years} rows with invalid Year values")
                        df = df.dropna(subset=['Year'])
                    
                    # Verify years are within a reasonable range
                    out_of_range = ((df['Year'] < 1980) | (df['Year'] > 2030)).sum()
                    if out_of_range > 0:
                        logger.warning(f"Found {out_of_range} rows with years outside reasonable range (1980-2030)")
                
                # Check for duplicates
                duplicates = df.duplicated(subset=['PRESIDENTE_MUNICIPAL', 'MUNICIPIO', 'Year'], keep='first').sum()
                if duplicates > 0:
                    logger.warning(f"Found {duplicates} duplicate candidate entries (keeping first occurrence)")
                    df = df.drop_duplicates(subset=['PRESIDENTE_MUNICIPAL', 'MUNICIPIO', 'Year'], keep='first')
                
                logger.info(f"Successfully cleaned data: {len(df)} valid candidates")
                return df
                
            except UnicodeDecodeError:
                # Try different encodings if UTF-8 fails
                for encoding in ['latin1', 'iso-8859-1', 'cp1252']:
                    try:
                        df = pd.read_csv(self.candidates_csv, encoding=encoding)
                        logger.info(f"Loaded {len(df)} candidates from CSV using {encoding} encoding")
                        
                        # Apply the same cleaning as above
                        # Essential columns that must have values
                        essential_columns = ['PRESIDENTE_MUNICIPAL', 'MUNICIPIO', 'Year']
                        
                        # Verify all essential columns exist
                        missing_columns = [col for col in essential_columns if col not in df.columns]
                        if missing_columns:
                            logger.error(f"Missing essential columns in CSV: {missing_columns}")
                            continue
                        
                        # Replace empty strings with NaN for consistent handling
                        df = df.replace('', pd.NA)
                        
                        # Drop rows with missing essential data
                        df = df.dropna(subset=essential_columns)
                        
                        # Convert string columns to strings (handles numeric values)
                        string_columns = [
                            'PRESIDENTE_MUNICIPAL', 'MUNICIPIO', 'ENTIDAD', 'PARTIDO', 
                            'PERIODO_FORMATO_ORIGINAL', 'SEXO'
                        ]
                        
                        for col in string_columns:
                            if col in df.columns:
                                # Convert to string, but handle NaN values first
                                df[col] = df[col].fillna('').astype(str)
                                # Convert empty strings back to NaN
                                df[col] = df[col].replace('', pd.NA)
                                # Strip whitespace
                                df[col] = df[col].str.strip()
                        
                        # Ensure Year is numeric
                        if 'Year' in df.columns:
                            df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
                            # Drop rows with invalid years
                            df = df.dropna(subset=['Year'])
                        
                        # Check for duplicates
                        df = df.drop_duplicates(subset=['PRESIDENTE_MUNICIPAL', 'MUNICIPIO', 'Year'], keep='first')
                        
                        logger.info(f"Successfully cleaned data: {len(df)} valid candidates")
                        return df
                    except Exception as e:
                        logger.warning(f"Failed to load with {encoding} encoding: {str(e)}")
                        continue
                        
                logger.error(f"Failed to load candidates CSV with any encoding")
            except Exception as e:
                logger.error(f"Error loading candidates CSV: {str(e)}")
                traceback.print_exc()
        
        return None

    def run_batch(self, candidates=None, max_candidates=None):
        """
        Run a batch of candidate processing with enhanced progress tracking.
        
        Args:
            candidates (list, optional): List of candidate dictionaries or DataFrame.
            max_candidates (int, optional): Maximum number of candidates to process.
            
        Returns:
            int: Batch ID
        """
        try:
            # Use provided candidates or all candidates
            if candidates is None:
                if self.candidates_data is None:
                    logger.error("No candidates data available")
                    return None
                
                all_candidates = self.candidates_data.to_dict('records')
            elif isinstance(candidates, pd.DataFrame):
                all_candidates = candidates.to_dict('records')
            else:
                all_candidates = candidates
                
            # Limit to max_candidates if specified
            if max_candidates and max_candidates > 0:
                candidates_to_process = all_candidates[:max_candidates]
            else:
                candidates_to_process = all_candidates
            
            # Filter out already completed candidates
            candidates_to_process = self.db.get_candidates_to_process(candidates_to_process)
            
            if not candidates_to_process:
                logger.info("No new candidates to process")
                return None
                
            total_candidates = len(candidates_to_process)
            logger.info(f"Starting batch with {total_candidates} candidates")
            
            # Create a new batch with configuration
            config = {
                'max_threads': self.max_threads,
                'article_threads': self.article_threads,
                'year_range': self.year_range,
                'use_oxylabs': self.use_oxylabs,
                'enhanced_search': self.enhanced_search,
            }
            
            batch_id = self.db.create_batch(total_candidates, config)
            
            # Add batch_id to each candidate
            for candidate in candidates_to_process:
                candidate['batch_id'] = batch_id
            
            # Process candidates concurrently
            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:
                future_to_candidate = {
                    executor.submit(self.process_candidate, candidate): candidate 
                    for candidate in candidates_to_process
                }
                
                completed_count = 0
                for future in future_to_candidate:
                    try:
                        candidate = future_to_candidate[future]
                        candidate_name = candidate.get('PRESIDENTE_MUNICIPAL', candidate.get('name'))
                        
                        result_count = future.result()
                        logger.info(f"Completed {candidate_name} with {result_count} results")
                        completed_count += 1
                        
                        # Update batch progress
                        progress_percent = (completed_count / total_candidates) * 100
                        logger.info(f"Batch progress: {progress_percent:.1f}% ({completed_count}/{total_candidates})")
                        
                        # Update batch status periodically
                        if completed_count % 10 == 0 or completed_count == total_candidates:
                            self.db.update_batch_status(
                                batch_id, 'IN_PROGRESS', completed_candidates=completed_count
                            )
                            
                    except Exception as e:
                        logger.error(f"Error with candidate {candidate.get('PRESIDENTE_MUNICIPAL', 'unknown')}: {str(e)}")
                        import traceback
                        traceback.print_exc()
            
            # Mark batch as completed
            self.db.update_batch_status(batch_id, 'COMPLETED', completed_candidates=completed_count)
            logger.info(f"Batch {batch_id} completed")
            
            # Create candidate profiles
            logger.info("Creating candidate profiles...")
            self.db.create_candidate_profiles()
            
            return batch_id
            
        except Exception as e:
            logger.error(f"Error running batch: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def process_candidate(self, candidate):
        """
        Process a single candidate with comprehensive error handling and data validation.
        
        Args:
            candidate (dict): Candidate information dictionary
            
        Returns:
            int: Number of articles found or 0 if processing failed
        """
        try:
            # Extract candidate information with data validation
            candidate_name = self._validate_string(candidate, 'PRESIDENTE_MUNICIPAL', 'name')
            municipality = self._validate_string(candidate, 'MUNICIPIO', 'municipality')
            target_year = self._safe_get_value(candidate, 'Year', 'target_year', numeric=True)
            
            # Verify essential fields are present
            if not candidate_name or not municipality or not target_year:
                logger.warning(f"Skipping candidate with missing essential data: {candidate}")
                return 0
            
            # Extract optional fields with validation
            state = self._validate_string(candidate, 'ENTIDAD', 'entidad')
            gender = self._validate_string(candidate, 'SEXO', 'gender')
            party = self._validate_string(candidate, 'PARTIDO', 'party')
            period = self._validate_string(candidate, 'PERIODO_FORMATO_ORIGINAL', 'period_format')
            batch_id = candidate.get('batch_id')
            
            # Create a clean candidate dictionary with validated data
            clean_candidate = {
                'name': candidate_name,
                'municipality': municipality,
                'target_year': target_year,
                'entidad': state,
                'gender': gender,
                'party': party,
                'period_format': period,
                'batch_id': batch_id
            }
            
            logger.info(f"Processing candidate: {candidate_name}, {municipality}, {target_year}")
            
            # Use enhanced search if enabled, otherwise use standard search
            if self.enhanced_search:
                results = self.search_engine.search_candidate_enhanced(
                    clean_candidate,
                    batch_id=batch_id
                )
            else:
                results = self.search_engine.search_candidate(
                    clean_candidate,
                    batch_id=batch_id
                )
            
            logger.info(f"Found {len(results)} articles for {candidate_name}")
            return len(results)
        
        except Exception as e:
            logger.error(f"Error processing candidate: {str(e)}")
            import traceback
            traceback.print_exc()
            return 0

    def _validate_string(self, candidate, primary_key, backup_key=None):
        """
        Validate and extract string value from candidate data.
        
        Args:
            candidate (dict): Candidate data
            primary_key (str): Primary key to try
            backup_key (str, optional): Backup key if primary isn't found
            
        Returns:
            str: Validated string or None if invalid
        """
        value = None
        
        # Try primary key first
        if primary_key in candidate and candidate[primary_key] is not None:
            value = candidate[primary_key]
        # Try backup key if primary key failed
        elif backup_key and backup_key in candidate and candidate[backup_key] is not None:
            value = candidate[backup_key]
        else:
            return None
        
        # Return None for pandas NA values
        if pd.isna(value):
            return None
            
        # For string values
        if isinstance(value, str):
            # Return None for empty strings after stripping
            stripped = value.strip()
            return stripped if stripped else None
        elif isinstance(value, (int, float)):
            # Convert numeric types to string
            return str(value)
        else:
            # Return None for other types
            return None

    def _safe_get_value(self, data, primary_key, backup_key=None, numeric=False):
        """
        Safely extract a value from a dictionary with comprehensive data validation.
        
        Args:
            data (dict): Dictionary to extract from
            primary_key (str): Primary key to look for
            backup_key (str, optional): Backup key if primary isn't found
            numeric (bool, optional): Whether to treat as numeric value
            
        Returns:
            Value or None if not found/invalid
        """
        value = None
        
        # Try primary key first
        if primary_key in data and data[primary_key] is not None:
            value = data[primary_key]
        # Try backup key if primary key failed
        elif backup_key and backup_key in data and data[backup_key] is not None:
            value = data[backup_key]
        else:
            return None
        
        # Return None for pandas NA values
        if pd.isna(value):
            return None
            
        # Validate based on type
        if numeric:
            # For numeric values
            try:
                # Try converting to int first
                return int(value)
            except (ValueError, TypeError):
                try:
                    # If that fails, try float
                    return float(value)
                except (ValueError, TypeError):
                    # If all conversion fails, return None
                    return None
        else:
            # For string values
            if isinstance(value, str):
                # Return None for empty strings after stripping
                stripped = value.strip()
                return stripped if stripped else None
            elif isinstance(value, (int, float)):
                # Convert numeric types to string
                return str(value)
            else:
                # Return None for other types
                return None

if __name__ == "__main__":
    main()